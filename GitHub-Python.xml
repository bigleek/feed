<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GitHub-Python</title>
    <link>https://mshibanami.github.io/GitHubTrendingRSS/daily/python.xml</link>
    <description>Summarized RSS feed for GitHub-Python</description>
    <atom:link href="https://mshibanami.github.io/GitHubTrendingRSS/daily/python.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title>lzhoang2801/OpCore-Simplify</title>
      <link>https://github.com/lzhoang2801/OpCore-Simplify</link>
      <description>&lt;p&gt;概要: OpCore Simplify 是一款专为简化 OpenCore EFI 创建而设计的工具，通过自动化关键设置流程与标准化配置，减少手动操作并提升 Hackintosh 安装的准确性。它支持广泛的硬件与 macOS 版本，集成 ACPI 修补和 Kext 管理，具备自动更新功能，并允许用户进行进一步定制，为用户提供一个可靠且高效的 EFI 构建平台。&lt;/p&gt;

&lt;h3&gt;核心要点:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;唯一基于完整硬件配置构建 OpenCore EFI 的工具&lt;/strong&gt;：与其它 Hackintosh 工具不同，OpCore Simplify 使用用户的实际硬件信息生成 EFI，提升兼容性与稳定性。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;全面硬件与 macOS 支持&lt;/strong&gt;：支持从 Intel Nehalem 到 AMD Ryzen 的多种处理器，以及 Intel iGPU、AMD APU/dGPU 和 NVIDIA 显卡，并兼容 macOS High Sierra 至 Tahoe。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;自动化 ACPI 修补与 Kext 检测&lt;/strong&gt;：根据硬件自动添加必要的 ACPI 补丁和 Kext，集成 SSDTTime 工具，并支持定制修补和设备配置。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;支持手动定制和优化配置&lt;/strong&gt;：用户可在默认设置基础上调整 ACPI 补丁、Kext 加载及 SMBIOS，解锁更多功能并提升系统性能。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;强调社区验证与更新文档&lt;/strong&gt;：提示用户应优先依赖 Hackintosh 社区和官方文档，而非 AI/LLM 提供的可能错误信息，并建议在每次构建前手动验证和测试配置。&lt;/li&gt;
&lt;/ul&gt;</description>
      <pubDate/>
      <guid>https://github.com/lzhoang2801/OpCore-Simplify</guid>
    </item>
    <item>
      <title>volcengine/MineContext</title>
      <link>https://github.com/volcengine/MineContext</link>
      <description>&lt;p&gt;概要: MineContext是一款基于上下文感知的开源AI助手，通过集成屏幕截图、多模态内容处理及上下文工程框架，主动收集并整理用户的数字环境信息，提供每日总结、待办事项、活动记录等智能内容，同时强调本地化数据处理和隐私保护，允许用户使用自有API密钥实现成本优化和灵活定制。&lt;/p&gt;

&lt;h3&gt;核心要点:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本地优先与隐私保护&lt;/strong&gt;：数据默认存储于本地，支持本地AI模型部署，避免依赖云端，保障用户隐私与安全。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;全面的上下文采集能力&lt;/strong&gt;：支持屏幕截图、多源文档、图片、视频、代码及多款应用程序数据的集成采集，覆盖工作、生活和创作场景。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自动化的智能交付与生成&lt;/strong&gt;：基于上下文工程架构，能主动推送每日/周总结、待办事项、提示信息等，提升信息处理效率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;技术支持与可扩展性&lt;/strong&gt;：采用Electron、React、TypeScript等技术栈构建，支持OpenAI、Doubao等LLM服务，并允许用户自定义模型，增强可配置性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;开源与社区驱动&lt;/strong&gt;：作为开源项目，MineContext鼓励开发者贡献代码，同时提供多渠道的社区支持，包括GitHub、WeChat和Discord。&lt;/li&gt;
&lt;/ul&gt;</description>
      <pubDate/>
      <guid>https://github.com/volcengine/MineContext</guid>
    </item>
    <item>
      <title>google/adk-python</title>
      <link>https://github.com/google/adk-python</link>
      <description>&lt;p&gt;概要: Agent Development Kit (ADK) 是一个开源、基于代码的 Python 工具包，旨在通过灵活且可控的方式构建、评估和部署复杂的 AI 代理，支持多种模型和部署环境，并提供丰富的工具生态系统、模块化架构以及社区驱动的扩展。&lt;/p&gt;  
&lt;h3&gt;核心要点:&lt;/h3&gt;  
&lt;ul&gt;  
  &lt;li&gt;&lt;strong&gt;模型与部署无关&lt;/strong&gt;：ADK 支持多种 AI 模型和部署环境，尤其优化于 Gemini，但保持兼容性与灵活性。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;代码优先开发&lt;/strong&gt;：通过 Python 直接定义代理逻辑、工具和流程，提升可测试性、版本管理和可定制性。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;多代理系统支持&lt;/strong&gt;：允许构建模块化的多代理系统，通过协调机制实现复杂任务的高效执行。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;社区扩展与贡献&lt;/strong&gt;：提供社区仓库扩展功能，支持第三方工具集成和部署脚本，并鼓励开发者参与贡献。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;内置开发界面&lt;/strong&gt;：包含测试、评估、调试和展示代理的开发界面，提升开发效率与可视化能力。&lt;/li&gt;  
&lt;/ul&gt;</description>
      <pubDate/>
      <guid>https://github.com/google/adk-python</guid>
    </item>
    <item>
      <title>thinking-machines-lab/tinker-cookbook</title>
      <link>https://github.com/thinking-machines-lab/tinker-cookbook</link>
      <description>&lt;p&gt;概要: Tinker为研究人员和开发者提供两个核心工具——tinker训练SDK与tinker-cookbook示例库，前者通过API简化分布式训练流程，后者包含多种应用场景的抽象模板和代码示例，支持从基础微调到高级强化学习等技术实现，并配套实用工具和社区协作机制，旨在降低LLM定制化训练的复杂性。&lt;/p&gt;  
&lt;h3&gt;核心要点:&lt;/h3&gt;  
&lt;ul&gt;  
  &lt;li&gt;&lt;strong&gt;双库架构：tinker为训练SDK，tinker-cookbook为示例库&lt;/strong&gt;，前者提供分布式训练能力，后者包含监督学习、强化学习等场景的代码模板。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;高级训练范式覆盖&lt;/strong&gt;：支持聊天监督学习、数学推理优化、偏好学习（RLHF全流程）、工具使用增强等多样化应用。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;实用工具链集成&lt;/strong&gt;：包含token结构转换、超参数计算、模型评估与基准测试对接等模块，提升训练效率。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;标准化的安装与授权流程&lt;/strong&gt;：需通过等待名单注册、API密钥配置及虚拟环境部署，确保安全与便捷性。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;社区驱动开发模式&lt;/strong&gt;：鼓励在私有测试后提交贡献，提供反馈渠道与学术引用规范。&lt;/li&gt;  
&lt;/ul&gt;</description>
      <pubDate/>
      <guid>https://github.com/thinking-machines-lab/tinker-cookbook</guid>
    </item>
    <item>
      <title>google-agentic-commerce/AP2</title>
      <link>https://github.com/google-agentic-commerce/AP2</link>
      <description>&lt;p&gt;概要: 本文本介绍了Agent Payments Protocol (AP2)，提供了一个包含代码示例和演示的仓库，旨在展示其在实现安全且可互操作的AI驱动支付方面的关键功能。AP2允许开发者使用多种工具构建智能代理，且提供了快速启动和运行场景的指导。&lt;/p&gt;  

&lt;h3&gt;核心要点:&lt;/h3&gt;  
&lt;ul&gt;  
  &lt;li&gt;&lt;strong&gt;AP2是一个用于实现安全、可互操作AI支付功能的协议框架。&lt;/strong&gt;&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;仓库中包含多个场景示例，用以展示AP2的核心组件，支持Python和Android开发。&lt;/strong&gt;&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;开发者可选用Google API Key或Vertex AI进行身份验证，两者均可通过环境变量或.env文件配置。&lt;/strong&gt;&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;运行场景需通过README.md中的指示执行，通常包括安装依赖及启动代理。&lt;/strong&gt;&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;AP2的类型定义位于src/ap2/types目录，目前可通过git直接安装，未来将发布PyPI包。&lt;/strong&gt;&lt;/li&gt;  
&lt;/ul&gt;</description>
      <pubDate/>
      <guid>https://github.com/google-agentic-commerce/AP2</guid>
    </item>
    <item>
      <title>MODSetter/SurfSense</title>
      <link>https://github.com/MODSetter/SurfSense</link>
      <description>&lt;p&gt;概要: SurfSense 是一款开源的 AI 研究代理工具，作为 NotebookLM、Perplexity 和 Glean 的替代方案，具备强大的跨平台集成能力，能够连接多种外部资源如搜索引擎、Slack、Notion、GitHub 等，并支持多格式文件上传、自然语言交互、引用式答案生成以及隐私保护和本地部署，同时具备快速生成播客的功能，适用于企业级研究与内容管理需求。&lt;/p&gt;

&lt;h3&gt;核心要点:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;高度可定制的私有 AI 研究代理&lt;/strong&gt;，集成多种外部数据源，增强企业级知识管理能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支持多文件格式上传与处理&lt;/strong&gt;，覆盖 50+ 种文档、图像及数据格式，适合多样化内容存储。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私与本地部署友好&lt;/strong&gt;，兼容 Ollama 本地 LLM，支持自托管且无需 API 密钥。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;快速播客生成能力&lt;/strong&gt;，可在 20 秒内将聊天内容转为 3 分钟播客，支持多种 TTS 服务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高级 RAG 技术&lt;/strong&gt;，结合语义和全文搜索，提升检索准确性和效率。&lt;/li&gt;
&lt;/ul&gt;</description>
      <pubDate/>
      <guid>https://github.com/MODSetter/SurfSense</guid>
    </item>
    <item>
      <title>HKUDS/LightRAG</title>
      <link>https://github.com/HKUDS/LightRAG</link>
      <description>&lt;p&gt;概要: LightRAG 通过集成 RAGAS 评估框架和 Langfuse 可观测性，实现了检索增强生成技术的性能优化与可追踪性。它支持多模态数据处理、大模型瓶颈突破、多存储后端兼容性增强（如Neo4J、PostgreSQL、MongoDB），并提供 WebUI 和异步 API 调用能力，同时支持实体关系图的编辑、合并、删除等灵活知识管理功能，以提升大型语言模型生成结果的准确性与实用性。&lt;/p&gt;  
&lt;h3&gt;核心要点:&lt;/h3&gt;  
&lt;ul&gt;  
&lt;li&gt;&lt;strong&gt;多模态集成&lt;/strong&gt;: 支持文本、图像、表格、公式等多样化格式处理，结合 RAG-Anything 实现一体化多模态检索增强生成。&lt;/li&gt;  
&lt;li&gt;&lt;strong&gt;RAGAS 评估框架&lt;/strong&gt;: 引入基于大语言模型的无参考评估，量化生成结果的全面性、多样性与赋能效果，提升系统分析效率。&lt;/li&gt;  
&lt;li&gt;&lt;strong&gt;存储后端优化&lt;/strong&gt;: 提供 Neo4J、PostgreSQL、MongoDB 等多存储方案，并支持通过命名空间实现跨实例数据隔离，确保生产环境可信和可扩展。&lt;/li&gt;  
&lt;li&gt;&lt;strong&gt;高性能检索模式&lt;/strong&gt;: 默认采用 "mix" 模式结合知识图谱与向量检索，支持异步 API 调用，并通过 reranker 模型提升混杂查询结果的相关性。&lt;/li&gt;  
&lt;li&gt;&lt;strong&gt;轻量化与扩展性&lt;/strong&gt;: 优化低显存 GPU 场景下模型部署，支持自定义实体关系合并策略，并具备智能增量更新与数据删除的缓存优化机制。&lt;/li&gt;  
&lt;/ul&gt;</description>
      <pubDate/>
      <guid>https://github.com/HKUDS/LightRAG</guid>
    </item>
    <item>
      <title>volcengine/verl</title>
      <link>https://github.com/volcengine/verl</link>
      <description>&lt;p&gt;概要: verl 是一个专为大型语言模型（LLMs）设计的灵活、高效且可应用于生产环境的强化学习（RL）训练库，其基于 HybridFlow 框架开源，支持多种 RLHF 算法与主流 LLM 框架整合，具备出色的吞吐量与资源利用能力，广泛适用于多模态、多任务及多轮对话的训练优化，并已在多个大型模型和会议中展示其技术成果。&lt;/p&gt;

&lt;h3&gt;核心要点:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;verl 是一个灵活、高效且可生产部署的 RL 训练库&lt;/strong&gt;，支持多种主流 RL 算法与模型框架兼容，如 PPO、GRPO、DAPO 等，并与 HuggingFace 和 ModelScope 的模型无缝集成。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;verl 支持高效的资源利用和多集群扩展能力&lt;/strong&gt;，通过灵活的设备映射策略和 3D-HybridEngine 实现模型重分片，显著减少内存冗余和通信开销。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;verl 在多模态和多任务场景中表现突出&lt;/strong&gt;，支持 VLMs、工具调用、多轮推理及多种奖励机制（如模型内奖励和函数奖励），并已在多个大型模型中实现 SOTA 性能。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;verl 与 vLLM、SGLang 等工具深度集成&lt;/strong&gt;，并持续优化，如支持 vLLM&gt;=0.8.2 和 FSDP2，提升吞吐与内存效率，同时逐步扩展对 AMD ROCm 的支持。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;verl 已被多个知名机构和团队采用并贡献&lt;/strong&gt;，包括 ByteDance、Alibaba Qwen、Tsinghua University 等，且持续在 ICML、EuroSys、PyTorch Day 等顶级会议中展示与更新。&lt;/li&gt;
&lt;/ul&gt;</description>
      <pubDate/>
      <guid>https://github.com/volcengine/verl</guid>
    </item>
    <item>
      <title>googlefonts/googlesans-code</title>
      <link>https://github.com/googlefonts/googlesans-code</link>
      <description>&lt;p&gt;概要: Google Sans Code 是一款专为编程环境设计的固定宽度字体家族，融合了 Google 品牌的视觉风格，提升了代码的可读性与清晰度，适用于如 Gemini 和 Android Studio 等产品。该字体经过精细调校，确保在小字号下字符仍然辨识明确，并支持多种语言和变体功能。&lt;/p&gt;  
&lt;h3&gt;核心要点:&lt;/h3&gt;  
&lt;ul&gt;  
  &lt;li&gt;&lt;strong&gt;专为编程设计的固定宽度字体&lt;/strong&gt;：优化代码编辑器和终端显示，提升可读性和清晰度。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;支持多语言和扩展拉丁字符集&lt;/strong&gt;：满足国际化开发需求，适应不同语言环境。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;Variable 字体技术&lt;/strong&gt;：提供从 300 到 800 的广泛字重范围，支持灵活的显示需求。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;CI/CD 自动化流程&lt;/strong&gt;：每次提交或 Pull Request 都会自动编译和测试，确保字体质量。&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;开源许可协议&lt;/strong&gt;：遵循 SIL Open Font License 1.1，允许自由使用和修改。&lt;/li&gt;  
&lt;/ul&gt;</description>
      <pubDate/>
      <guid>https://github.com/googlefonts/googlesans-code</guid>
    </item>
    <item>
      <title>microsoft/call-center-ai</title>
      <link>https://github.com/microsoft/call-center-ai</link>
      <description>&lt;p&gt;概要: 这是一个基于Azure和OpenAI GPT的智能客服解决方案，支持通过API调用或预配置电话号码发起语音通话，并可处理保险、IT支持、客户服务等多种场景，具有实时对话、多语言支持、自动化信息收集与存储、语音与文本转换、语音识别、内容安全及个性化功能，系统采用云原生架构部署，以实现灵活、可扩展且低成本的运营，同时提供丰富的配置选项和实时监控能力，适用于快速测试和未来生产部署。&lt;/p&gt;

&lt;h3&gt;核心要点:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;AI驱动的自动化电话服务&lt;/strong&gt;: 可通过API或预设号码发起呼叫，集成语音、短信和实时对话流处理，提升用户体验和效率。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;定制化与扩展性强&lt;/strong&gt;: 支持自定义话术、数据结构、语音识别参数和内容安全机制，适应不同业务需求。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;支持多种语言与语音风格&lt;/strong&gt;: 可根据不同语言配置语音识别与生成，确保不同地区的用户获得一致服务。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;云原生架构部署&lt;/strong&gt;: 基于Azure的容器化、无服务器架构，结合语音与智能服务，支持弹性扩展和低成本运营。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;全面的监控与优化&lt;/strong&gt;: 通过Azure Application Insights监控系统性能和LLM延迟，支持A/B测试和持续改进。&lt;/li&gt;
&lt;/ul&gt;</description>
      <pubDate/>
      <guid>https://github.com/microsoft/call-center-ai</guid>
    </item>
    <item>
      <title>RLinf/RLinf</title>
      <link>https://github.com/RLinf/RLinf</link>
      <description/>
      <pubDate/>
      <guid>https://github.com/RLinf/RLinf</guid>
    </item>
    <item>
      <title>GibsonAI/Memori</title>
      <link>https://github.com/GibsonAI/Memori</link>
      <description/>
      <pubDate/>
      <guid>https://github.com/GibsonAI/Memori</guid>
    </item>
    <item>
      <title>AtsushiSakai/PythonRobotics</title>
      <link>https://github.com/AtsushiSakai/PythonRobotics</link>
      <description/>
      <pubDate/>
      <guid>https://github.com/AtsushiSakai/PythonRobotics</guid>
    </item>
    <item>
      <title>Zie619/n8n-workflows</title>
      <link>https://github.com/Zie619/n8n-workflows</link>
      <description/>
      <pubDate/>
      <guid>https://github.com/Zie619/n8n-workflows</guid>
    </item>
    <item>
      <title>yeongpin/cursor-free-vip</title>
      <link>https://github.com/yeongpin/cursor-free-vip</link>
      <description/>
      <pubDate/>
      <guid>https://github.com/yeongpin/cursor-free-vip</guid>
    </item>
    <item>
      <title>MustardChef/WSABuilds</title>
      <link>https://github.com/MustardChef/WSABuilds</link>
      <description/>
      <pubDate/>
      <guid>https://github.com/MustardChef/WSABuilds</guid>
    </item>
    <item>
      <title>sansan0/TrendRadar</title>
      <link>https://github.com/sansan0/TrendRadar</link>
      <description/>
      <pubDate/>
      <guid>https://github.com/sansan0/TrendRadar</guid>
    </item>
  </channel>
</rss>